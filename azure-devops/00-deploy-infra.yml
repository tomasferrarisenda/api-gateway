name: deploy-infra

trigger: none

pool:
  vmImage: 'ubuntu-latest' 
  # If you are using a self-hosted agent, comment out the previous line and uncomment the following three
  # name: <agent-pool-name> # Insert here the name of the agent pool you created
  # demands:
  #   - agent.name -equals <agent-name> # Insert here the name of the agent you created

variables:
  - group: aws-keys
  - name: AWS_REGION
    value: us-east-1  # This value was modified by the initial-setup python script
  # - name: DOMAIN
  #   value: tferrari.com # This value was modified by the initial-setup python script

steps:
# In this case it's necessary to specify the checkout with the persistCredential options set to true. This will enable us to push the changes to the repo.
# - checkout: self
#   persistCredentials: true

- task: TerraformInstaller@1
  displayName: Install terraform
  inputs:
    terraformVersion: '1.4.6'

- task: TerraformCLI@1
  displayName: 'Terraform Init Backend'
  inputs:
    command: init
    workingDirectory: 'terraform/backend'
    providerServiceAws: 'aws' 
    providerAwsRegion: $(AWS_REGION) 

# This step will fail if the infrastructure was successfully deployed in a prevous build
- task: TerraformCLI@1
  displayName: 'Terraform Apply Backend'
  inputs:
    command: apply
    workingDirectory: 'terraform/backend'
    providerServiceAws: 'aws' 
    providerAwsRegion: $(AWS_REGION) 
  # continueOnError: true

- script: cp terraform/backend/terraform.tfstate terraform/aws
  displayName: 'Move terraform.tfstate to /terraform/aws/ directory'
  condition: succeeded() # This will make this step be skipped if the previous step failed

- task: TerraformCLI@1
  displayName: 'Terraform Init Infra'
  condition: always()
  inputs:
    command: init
    workingDirectory: 'terraform/aws'
    backendType: aws 
    backendServiceAws: aws 
    providerServiceAws: 'aws' 
    providerAwsRegion: $(AWS_REGION) 
    commandOptions: '-force-copy'

- task: TerraformCLI@1
  displayName: 'Terraform Apply Infra'
  condition: always()
  inputs:
    command: apply
    workingDirectory: 'terraform/aws'
    providerServiceAws: 'aws' 
    providerAwsRegion: $(AWS_REGION)

# We update the local repo with a pull just in case there have been any recent modifications
# - script: |
#     git pull origin main
#   displayName: Update repo

# - script: |
#     echo "$(terraform -chdir=terraform/aws output -raw elasticache_dev_primary_endpoint_address) # This value was modified by the deploy-infra pipeline" > elasticache_dev_endpoint.txt
#     echo "$(terraform -chdir=terraform/aws output -raw elasticache_stage_primary_endpoint_address) # This value was modified by the deploy-infra pipeline" > elasticache_stage_endpoint.txt
#     echo "$(terraform -chdir=terraform/aws output -raw elasticache_prod_primary_endpoint_address) # This value was modified by the deploy-infra pipeline" > elasticache_prod_endpoint.txt
#   displayName: 'Save ElastiCache DBs endpoints'
#   condition: always()

# - script: |
#     sed -i "s/redis_host:.*/redis_host: $(cat elasticache_dev_endpoint.txt | tr -d '\n')/g" helm/my-app/backend/environments/values-dev.yaml
#     sed -i "s/redis_host:.*/redis_host: $(cat elasticache_stage_endpoint.txt | tr -d '\n')/g" helm/my-app/backend/environments/values-stage.yaml
#     sed -i "s/redis_host:.*/redis_host: $(cat elasticache_prod_endpoint.txt | tr -d '\n')/g" helm/my-app/backend/environments/values-prod.yaml
#   displayName: 'Modify values for each environment'
#   condition: always()

# - script: |
#     git config --global user.email "AzureDevOps@Build&DeployAppPipeline.com"
#     git config --global user.name "Azure DevOps - Build & Deploy App Pipeline"
#     git checkout -b main
#     git add helm/my-app/backend/environments/
#     git commit -m "ElastiCache endpoints updated in environments values.yaml files by Azure DevOps"
#     git push --set-upstream origin main
#   displayName: 'Push changes to GitHub'
#   condition: always()

- script: |
    echo "Assuming you are on the root directory of the automate-all-the-things repo, run this:" > ssh-to-bastion.txt
    echo "chmod 400 terraform/aws/templates/private-key" >> ssh-to-bastion.txt
    echo "ssh -i terraform/aws/templates/private-key ubuntu@$(terraform -chdir=terraform/aws output -raw ssh_host)" >> ssh-to-bastion.txt
  displayName: 'Save SSH command'
  condition: always()

- task: PublishBuildArtifacts@1
  displayName: 'Export SSH command'
  condition: always()
  inputs:
    PathtoPublish: 'ssh-to-bastion.txt'
    ArtifactName: 'SSH to bastion'
    publishLocation: 'Container'

# - script: | 
#     echo "$(terraform -chdir=terraform/aws output -raw base_url)" > api_key.txt
#     echo "$(terraform -chdir=terraform/aws output -raw api_key)" >> api_key.txt
#   displayName: 'Configure AWS Profile'
#   condition: always()

- script: | 
    echo "curl -X GET $(terraform -chdir=terraform/aws output -raw base_url)hello -H 'x-api-key: $(terraform -chdir=terraform/aws output -raw api_key)'" > api_gateway_commands.txt
    echo "" >> api_gateway_commands.txt
    echo "curl -X POST $(terraform -chdir=terraform/aws output -raw base_url)myresource -H 'x-api-key: $(terraform -chdir=terraform/aws output -raw api_key)' -H 'Content-Type: application/json' -d '{\"number1\": 10, \"number2\": 56}'" >> api_gateway_commands.txt

  displayName: 'Get aapi gw commands'
  condition: always()


- task: PublishBuildArtifacts@1
  displayName: 'Export api_gateway_commands'
  condition: always()
  inputs:
    PathtoPublish: 'api_gateway_commands.txt'
    ArtifactName: 'api_gateway_commands'
    publishLocation: 'Container'


- script: | 
    echo "$(terraform -chdir=terraform/aws output -raw ssh_host_public_dns):3000/tshirt" > ec2_api.txt
  displayName: 'get ec2 url'
  condition: always()


- task: PublishBuildArtifacts@1
  displayName: 'Export ec2_api'
  condition: always()
  inputs:
    PathtoPublish: 'ec2_api.txt'
    ArtifactName: 'ec2_api'
    publishLocation: 'Container'

# - script: | 
#     mkdir ~/.aws
#     echo -e "[default]\naws_access_key_id = $(aws_access_key_id)\naws_secret_access_key = $(aws_secret_access_key)" > ~/.aws/credentials
#     echo -e "[default]\nregion = $(AWS_REGION)"> ~/.aws/config 
#   displayName: 'Configure AWS Profile'
#   condition: always()

# - script: |
#     ZONE_ID=$(aws route53 list-hosted-zones-by-name --output json --dns-name "$(DOMAIN)" |  jq -r '.HostedZones[0].Id' | cut -d'/' -f3)
#     NS_VALUES=$(aws route53 list-resource-record-sets --hosted-zone-id $ZONE_ID | jq -r '.ResourceRecordSets[] | select(.Type=="NS") .ResourceRecords[].Value')
#     FORMATTED_NS_VALUES=$(echo "$NS_VALUES" | sed 's/\.$//;s/^/Name=/')
#     SINGLE_LINE_NS_VALUES=$(echo "${FORMATTED_NS_VALUES}" | tr '\n' ' ')
#     echo $SINGLE_LINE_NS_VALUES
#     aws route53domains update-domain-nameservers --region us-east-1 --domain-name $(DOMAIN) --nameservers $(echo $SINGLE_LINE_NS_VALUES)

#     # Enabling DNSSEC signing will fail if we've just updated our domain's name servers, so we'll keep trying every one minute.
#     while true; do
#       set +e
#       aws route53 enable-hosted-zone-dnssec --region us-east-1 --hosted-zone-id $ZONE_ID
#       result=$?
#       set -e
#       if [ $result -ne 0 ]; then
#         echo "Attempt failed. Waiting for 1 minute before retrying..."
#         sleep 60
#       else
#         echo "DNSSEC signing enabled successfully."
#         break
#       fi
#     done

#     aws --region us-east-1 route53 get-dnssec --hosted-zone-id $ZONE_ID | jq -r '.KeySigningKeys[0].PublicKey' > public-signing-key.txt
#   displayName: 'Configure domain registrar name servers and enable DNSSEC signing on hosted zone'
#   condition: always()

# - task: PublishBuildArtifacts@1
#   displayName: 'Export SSH command'
#   condition: always()
#   inputs:
#     PathtoPublish: 'public-signing-key.txt'
#     ArtifactName: 'Public singning key'
#     publishLocation: 'Container'